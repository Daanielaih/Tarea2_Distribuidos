# -*- coding: utf-8 -*-
"""Distri base

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-RNSHVe0UionjJffcPi9eCuppIxSaqV

# Respondedor preguntitas

Modelo de calidad: Sim de coseno
"""

import pandas as pd
import google.generativeai as genai
import numpy as np
import random


genai.configure(api_key="AIzaSyA5aNjtIegCftM67zsC8ahEgGlEISbcmVc")

#dataset
df = pd.read_csv(
    "test.csv",
    header=None,
    quoting=1,
    escapechar="\\",
    on_bad_lines="skip",
    engine="python"
)

#renombrar columnas
df = df.rename(columns={
    0: "class_index",
    1: "question_title",
    2: "question_content",
    3: "best_answer"
})

#fila aleatoria
fila = random.randint(0, len(df) - 1)
titulo = df.iloc[fila]["question_title"]
pregunta = df.iloc[fila]["question_content"]
respuesta_humana = df.iloc[fila]["best_answer"]


model = genai.GenerativeModel("gemini-2.5-flash")

#titulo o titulo+contenido dependiendo del caso
if pd.isna(pregunta) or pregunta.strip() == "":
    prompt = f"Responde de forma clara y breve a la siguiente pregunta:\n\nTítulo: {titulo}"
else:
    prompt = f"Responde de forma clara y breve a la siguiente pregunta:\n\nTítulo: {titulo}\nPregunta: {pregunta}"

#Consultar al llm
respuesta_llm = model.generate_content(prompt).text

#función para similitud
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

#obtener embeddings
embed_model = "models/embedding-001"
embedding_humana = genai.embed_content(model=embed_model, content=respuesta_humana)["embedding"]
embedding_llm = genai.embed_content(model=embed_model, content=respuesta_llm)["embedding"]

#calcular similitud
score = cosine_similarity(embedding_humana, embedding_llm)

#resultados
print("Título:", titulo)
print("Pregunta:", pregunta)
print("\nRespuesta humana:", respuesta_humana)
print("\nRespuesta LLM:", respuesta_llm)
print(f"\n Similitud (coseno): {score:.4f}")

"""# Simulación de consultas

Opciones de distribuciones
1. Uniforme (discreta)

Cómo funciona: Cada interarrival (tiempo entre llegadas) se elige al azar dentro de un rango fijo, por ejemplo entre 1 y 5 segundos.




3. Exponencial (continua, usada en procesos de Poisson)

Ejemplo: rng.exponential(scale=1/lam) → valores como 0.2, 1.5, 3.1 segundos.

Características:

Es la distribución clásica para tiempos entre llegadas en un proceso de Poisson.

Produce muchos intervalos cortos, pero ocasionalmente intervalos largos.

 Adecuada para: simulaciones que quieren reflejar tráfico de usuarios real en sistemas (web, redes).

**Voy a usar uniforme y exponencial**
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta

class TrafficGenerator:
    def __init__(self, dataset, distribution="uniform", lam=5, uniform_low=1, uniform_high=5, seed=42, start_time=None):
        """
        dataset: DataFrame con preguntas (columnas: 'question_title', 'question_content')
        distribution: "uniform" o "exponential"
        lam: parámetro λ para distribución exponencial (media = 1/λ)
        uniform_low, uniform_high: rango para distribución uniforme
        seed: semilla para reproducibilidad
        start_time: datetime inicial para los timestamps (opcional)
        """
        self.dataset = dataset.reset_index(drop=True)
        self.distribution = distribution
        self.lam = lam
        self.uniform_low = uniform_low
        self.uniform_high = uniform_high
        self.rng = np.random.default_rng(seed)
        self.start_time = start_time

    def _sample_interarrival(self):
        """Genera un interarrival basado en la distribución seleccionada"""
        if self.distribution == "uniform":
            return self.rng.integers(self.uniform_low, self.uniform_high)
        elif self.distribution == "exponential":
            if self.lam <= 0:
                raise ValueError("λ debe ser > 0 para distribución exponencial")
            return self.rng.exponential(scale=1.0/self.lam)
        else:
            raise ValueError("Distribución no soportada: usa 'uniform' o 'exponential'")

    def generate_queries(self, n_queries=100):
        """
        Genera una lista de consultas simuladas al sistema
        Cada consulta contiene:
        - timestamp simulado
        - índice de pregunta
        - título
        - contenido
        """
        queries = []
        current_time = 0

        for _ in range(n_queries):
            # calcular tiempo de arribo
            inter_arrival = self._sample_interarrival()
            current_time += inter_arrival

            # elegir aleatoriamente una pregunta del dataset
            idx = self.rng.integers(0, len(self.dataset))
            row = self.dataset.iloc[idx]

            # definir timestamp (float o datetime)
            if self.start_time:
                timestamp = self.start_time + timedelta(seconds=float(current_time))
            else:
                timestamp = float(current_time)

            queries.append({
                "timestamp": timestamp,
                "index": int(idx),
                "title": row["question_title"],
                "content": row["question_content"]
            })

        return pd.DataFrame(queries)

# cargar dataset
df = pd.read_csv("test.csv", names=["class_index","question_title","question_content","best_answer"])

from datetime import datetime

# Generador con uniforme
tg_uniform = TrafficGenerator(df, distribution="uniform", uniform_low=1, uniform_high=5, seed=123)
traffic_uniform = tg_uniform.generate_queries(10)
print("Uniforme:\n", traffic_uniform.head())

# Generador con exponencial
tg_exp = TrafficGenerator(df, distribution="exponential", lam=0.5, seed=123)
traffic_exp = tg_exp.generate_queries(10)
print("\nExponencial:\n", traffic_exp.head())

"""# Cache"""

import time
from collections import OrderedDict

class Cache:
    def __init__(self, max_size=10, policy="LRU", ttl=None):
        """
        max_size: tamaño máximo del caché
        policy: "LRU" o "FIFO"
        ttl: tiempo de vida (en segundos). Si None → sin TTL
        """
        self.max_size = max_size
        self.policy = policy
        self.ttl = ttl
        self.store = OrderedDict()
        self.hits = 0
        self.misses = 0

    def _is_expired(self, key):
        """Verifica si la entrada ha expirado por TTL"""
        if self.ttl is None:
            return False
        value, timestamp = self.store[key]
        return (time.time() -   timestamp) > self.ttl

    def get(self, key):
        if key in self.store:
            if self._is_expired(key):
                # si expiró, se elimina y cuenta como miss
                del self.store[key]
                self.misses += 1
                return None
            self.hits += 1
            if self.policy == "LRU":
                self.store.move_to_end(key)
            return self.store[key][0]  # devolvemos solo el valor
        else:
            self.misses += 1
            return None

    def put(self, key, value):
        timestamp = time.time()
        if key in self.store:
            if self.policy == "LRU":
                self.store.move_to_end(key)
        else:
            if len(self.store) >= self.max_size:
                self.store.popitem(last=False)
        self.store[key] = (value, timestamp)

    def stats(self):
        total = self.hits + self.misses
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hits / total if total > 0 else 0,
            "miss_rate": self.misses / total if total > 0 else 0
        }

# Crear caché con TTL de 5 segundos
cache = Cache(max_size=3, policy="LRU", ttl=5)

cache.put("a", "respuesta A")
time.sleep(2)
print(cache.get("a"))  #  todavía existe

time.sleep(4)
print(cache.get("a"))  #  expiró (devuelve None)

print(cache.stats())

"""# Base de datos"""

import sqlite3

# Conexión a la base de datos (o crea una nueva)
conn = sqlite3.connect("queries.db")

# Crear tabla si no existe
c = conn.cursor()
c.execute('''
    CREATE TABLE IF NOT EXISTS queries (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        question_index INTEGER,
        question_title TEXT,
        question_content TEXT,
        human_answer TEXT,
        llm_answer TEXT,
        similarity_score REAL,
        count INTEGER DEFAULT 1
    )
''')
conn.commit()

def insert_or_update(conn, query):
    """
    Inserta o actualiza una entrada en la BD.

    query: diccionario con keys
        'index', 'title', 'content', 'human_answer', 'llm_answer', 'score'
    """
    c = conn.cursor()

    # Verificar si la pregunta ya existe
    c.execute("SELECT id, count FROM queries WHERE question_index=?", (query['index'],))
    row = c.fetchone()

    if row:
        # Actualizar contador, última respuesta LLM y score
        c.execute('''
            UPDATE queries
            SET count = ?, llm_answer = ?, similarity_score = ?
            WHERE id = ?
        ''', (row[1]+1, query['llm_answer'], query['score'], row[0]))
    else:
        # Insertar nueva fila
        c.execute('''
            INSERT INTO queries (question_index, question_title, question_content,
                                 human_answer, llm_answer, similarity_score, count)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (query['index'], query['title'], query['content'],
              query['human_answer'], query['llm_answer'], query['score'], 1))

    conn.commit()

# Suponiendo que ya tienes df cargado
gen = TrafficGenerator(df, distribution="uniform", seed=None)  # None = aleatorio
traffic_df = gen.generate_queries(n_queries=5)  # genera 20 consultas simuladas

# Conexión ya creada
c = conn.cursor()

# Obtener todos los índices ya guardados
c.execute("SELECT question_index FROM queries")
existing_indices = set(row[0] for row in c.fetchall())

# traffic_df es tu DataFrame generado
traffic_df = traffic_df[~traffic_df['index'].isin(existing_indices)]

for _, query in traffic_df.iterrows():
    key = query['index']

    # Revisar caché
    llm_answer = cache.get(key)
    if llm_answer is None:
        prompt = str(query['title']) + "\n" + str(query['content'])
        try:
            response = model.generate_content(prompt)
            llm_answer = response.text if response.text else "Contenido bloqueado"
        except ValueError:
            llm_answer = "Contenido bloqueado"
        cache.put(key, llm_answer)

    # Calcular embeddings para esta pregunta
    embedding_humana = genai.embed_content(
        model="models/embedding-001",
        content=str(df.loc[key, "best_answer"])
    )["embedding"]

    embedding_llm = genai.embed_content(
        model="models/embedding-001",
        content=llm_answer
    )["embedding"]

    # Calcular similitud
    score = cosine_similarity(embedding_humana, embedding_llm)

    # Guardar en BD
    query_data = {
        "index": key,
        "title": query['title'],
        "content": query['content'],
        "human_answer": df.loc[key, "best_answer"],
        "llm_answer": llm_answer,
        "score": score
    }
    insert_or_update(conn, query_data)

import pandas as pd
pd.read_sql("SELECT * FROM queries", conn)